[INFO] Scanning for projects...
[INFO] Inspecting build with total of 1 modules...
[INFO] Installing Nexus Staging features:
[INFO]   ... total of 1 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin
[INFO] 
[INFO] --------------< com.snowflake:snowflake-kafka-connector >---------------
[INFO] Building Snowflake Kafka Connector 1.7.1
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] Downloading from confluent: https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-converter/5.5.1/kafka-connect-avro-converter-5.5.1.jar
[INFO] Downloading from confluent: https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-data/5.5.1/kafka-connect-avro-data-5.5.1.jar
[INFO] Downloading from confluent: https://packages.confluent.io/maven/org/mockito/mockito-core/2.20.1/mockito-core-2.20.1.jar
[INFO] Downloading from confluent: https://packages.confluent.io/maven/io/confluent/common-utils/5.5.1/common-utils-5.5.1.jar
[INFO] Downloading from confluent: https://packages.confluent.io/maven/net/bytebuddy/byte-buddy/1.8.13/byte-buddy-1.8.13.jar
[INFO] Downloaded from confluent: https://packages.confluent.io/maven/io/confluent/common-utils/5.5.1/common-utils-5.5.1.jar (18 kB at 54 kB/s)
[INFO] Downloading from confluent: https://packages.confluent.io/maven/net/bytebuddy/byte-buddy-agent/1.8.13/byte-buddy-agent-1.8.13.jar
[INFO] Downloaded from confluent: https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-converter/5.5.1/kafka-connect-avro-converter-5.5.1.jar (8.5 kB at 25 kB/s)
[INFO] Downloading from confluent: https://packages.confluent.io/maven/org/apache/kafka/connect-json/0.9.0.0/connect-json-0.9.0.0.jar
[INFO] Downloaded from confluent: https://packages.confluent.io/maven/io/confluent/kafka-connect-avro-data/5.5.1/kafka-connect-avro-data-5.5.1.jar (43 kB at 109 kB/s)
[INFO] Downloading from cloudera-repo: https://repository.cloudera.com/content/repositories/releases/org/mockito/mockito-core/2.20.1/mockito-core-2.20.1.jar
[INFO] Downloading from cloudera-repo: https://repository.cloudera.com/content/repositories/releases/net/bytebuddy/byte-buddy/1.8.13/byte-buddy-1.8.13.jar
[INFO] Downloading from cloudera-repo: https://repository.cloudera.com/content/repositories/releases/net/bytebuddy/byte-buddy-agent/1.8.13/byte-buddy-agent-1.8.13.jar
[INFO] Downloading from cloudera-repo: https://repository.cloudera.com/content/repositories/releases/org/apache/kafka/connect-json/0.9.0.0/connect-json-0.9.0.0.jar
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/org/mockito/mockito-core/2.20.1/mockito-core-2.20.1.jar
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/net/bytebuddy/byte-buddy/1.8.13/byte-buddy-1.8.13.jar
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/net/bytebuddy/byte-buddy-agent/1.8.13/byte-buddy-agent-1.8.13.jar
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/org/apache/kafka/connect-json/0.9.0.0/connect-json-0.9.0.0.jar
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/net/bytebuddy/byte-buddy-agent/1.8.13/byte-buddy-agent-1.8.13.jar (42 kB at 773 kB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/net/bytebuddy/byte-buddy/1.8.13/byte-buddy-1.8.13.jar (3.0 MB at 17 MB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/kafka/connect-json/0.9.0.0/connect-json-0.9.0.0.jar (35 kB at 148 kB/s)
[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/mockito/mockito-core/2.20.1/mockito-core-2.20.1.jar (561 kB at 1.8 MB/s)
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ snowflake-kafka-connector ---
[INFO] Deleting /home/gabsko/breaking-updates/target
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.2:prepare-agent (pre-unit-test) @ snowflake-kafka-connector ---
[INFO] argLine set to -javaagent:/home/gabsko/.m2/repository/org/jacoco/org.jacoco.agent/0.8.2/org.jacoco.agent-0.8.2-runtime.jar=destfile=/home/gabsko/breaking-updates/target/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ snowflake-kafka-connector ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ snowflake-kafka-connector ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 47 source files to /home/gabsko/breaking-updates/target/classes
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/records/AvroConverterConfig.java: Some input files use or override a deprecated API.
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/records/AvroConverterConfig.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ snowflake-kafka-connector ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ snowflake-kafka-connector ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 35 source files to /home/gabsko/breaking-updates/target/test-classes
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/InternalStageIT.java: Some input files use or override a deprecated API.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/InternalStageIT.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/ConnectionServiceIT.java: Some input files use unchecked or unsafe operations.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/ConnectionServiceIT.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.0:test (default-test) @ snowflake-kafka-connector ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.snowflake.kafka.connector.ConnectorConfigTest
18-01-2023 10:27:06 main ERROR BufferThreshold:179 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
18-01-2023 10:27:06 main ERROR BufferThreshold:156 - 
[SF_KAFKA_CONNECTOR] buffer.flush.time is 0, it should be greater than 1
18-01-2023 10:27:06 main ERROR Utils:551 - 
[SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: $@#$#@%^$12312
18-01-2023 10:27:06 main ERROR StreamingUtils:218 - 
[SF_KAFKA_CONNECTOR] Config:value.converter has provided value:com.snowflake.kafka.connector.records.SnowflakeJsonConverter. If ingestionMethod is:snowpipe_streaming, Snowflake Custom Converters are not allowed. 
[SF_KAFKA_CONNECTOR] Invalid Converters:[com.snowflake.kafka.connector.records.SnowflakeJsonConverter, com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry, com.snowflake.kafka.connector.records.SnowflakeAvroConverter]
18-01-2023 10:27:06 main ERROR Utils:404 - 
[SF_KAFKA_CONNECTOR] snowflake.url.name cannot be empty.
18-01-2023 10:27:06 main ERROR BufferThreshold:214 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
18-01-2023 10:27:06 main ERROR BufferThreshold:179 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
18-01-2023 10:27:06 main ERROR StreamingUtils:199 - 
[SF_KAFKA_CONNECTOR] Kafka config:snowflake.ingestion.method error:Invalid value invalid for configuration errors.tolerance: String must be one of: none, all
18-01-2023 10:27:06 main ERROR StreamingUtils:159 - 
[SF_KAFKA_CONNECTOR] Config:snowflake.role.name should be present if ingestionMethod is:snowpipe_streaming
18-01-2023 10:27:06 main ERROR Utils:425 - 
[SF_KAFKA_CONNECTOR] Kafka provider config error:Unsupported provider name: Something_which_is_not_supported. Supported are: unknown,self_hosted,confluent
18-01-2023 10:27:06 main ERROR BufferThreshold:232 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records should be a positive integer. Provided:adssadsa
18-01-2023 10:27:06 main ERROR BufferThreshold:191 - 
[SF_KAFKA_CONNECTOR] buffer.size.bytes is too low at 0. It must be 1 or greater.
18-01-2023 10:27:06 main ERROR BufferThreshold:200 - 
[SF_KAFKA_CONNECTOR] Config buffer.size.bytes should be an integer. Provided:afdsa
18-01-2023 10:27:06 main ERROR StreamingUtils:199 - 
[SF_KAFKA_CONNECTOR] Kafka config:snowflake.ingestion.method error:Invalid value invalid_value for configuration snowflake.ingestion.method: String must be one of: snowpipe, snowpipe_streaming
18-01-2023 10:27:06 main ERROR Utils:391 - 
[SF_KAFKA_CONNECTOR] snowflake.private.key cannot be empty.
18-01-2023 10:27:06 main ERROR StreamingUtils:218 - 
[SF_KAFKA_CONNECTOR] Config:key.converter has provided value:com.snowflake.kafka.connector.records.SnowflakeJsonConverter. If ingestionMethod is:snowpipe_streaming, Snowflake Custom Converters are not allowed. 
[SF_KAFKA_CONNECTOR] Invalid Converters:[com.snowflake.kafka.connector.records.SnowflakeJsonConverter, com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry, com.snowflake.kafka.connector.records.SnowflakeAvroConverter]
18-01-2023 10:27:06 main ERROR Utils:412 - 
[SF_KAFKA_CONNECTOR] Proxy settings error: 
18-01-2023 10:27:06 main ERROR Utils:348 - 
[SF_KAFKA_CONNECTOR] name is empty or invalid. It should match Snowflake object identifier syntax. Please see the documentation.
18-01-2023 10:27:06 main ERROR Utils:412 - 
[SF_KAFKA_CONNECTOR] Proxy settings error: 
18-01-2023 10:27:06 main ERROR Utils:398 - 
[SF_KAFKA_CONNECTOR] snowflake.user.name cannot be empty.
18-01-2023 10:27:06 main ERROR StreamingUtils:199 - 
[SF_KAFKA_CONNECTOR] Kafka config:snowflake.ingestion.method error:Invalid value invalid for configuration errors.log.enable: String must be one of: true, false
18-01-2023 10:27:06 main ERROR BufferThreshold:214 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
18-01-2023 10:27:06 main ERROR BufferThreshold:165 - 
[SF_KAFKA_CONNECTOR] buffer.flush.time should be an integer. Invalid integer was provided:fdas
18-01-2023 10:27:06 main ERROR BufferThreshold:232 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records should be a positive integer. Provided:adssadsa
18-01-2023 10:27:06 main ERROR BufferThreshold:224 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is -1, it should at least 1
18-01-2023 10:27:06 main ERROR BufferThreshold:140 - 
[SF_KAFKA_CONNECTOR] Config buffer.flush.time is empty
18-01-2023 10:27:06 main ERROR BufferThreshold:165 - 
[SF_KAFKA_CONNECTOR] buffer.flush.time should be an integer. Invalid integer was provided:fdas
18-01-2023 10:27:06 main ERROR Utils:459 - 
[SF_KAFKA_CONNECTOR] Delivery Guarantee config:delivery.guarantee error:Unsupported Delivery Guarantee Type: INVALID. Supported are: at_least_once,exactly_once
18-01-2023 10:27:06 main ERROR Utils:436 - 
[SF_KAFKA_CONNECTOR] Kafka config:behavior.on.null.values error:Invalid value invalid for configuration behavior.on.null.values: String must be one of: default, ignore
18-01-2023 10:27:06 main ERROR StreamingUtils:176 - 
[SF_KAFKA_CONNECTOR] Config:delivery.guarantee should be:exactly_once if ingestion method is:snowpipe_streaming
18-01-2023 10:27:06 main ERROR BufferThreshold:224 - 
[SF_KAFKA_CONNECTOR] Config buffer.count.records is -1, it should at least 1
18-01-2023 10:27:06 main ERROR Utils:385 - 
[SF_KAFKA_CONNECTOR] snowflake.schema.name cannot be empty.
18-01-2023 10:27:06 main ERROR BufferThreshold:140 - 
[SF_KAFKA_CONNECTOR] Config buffer.flush.time is empty
18-01-2023 10:27:06 main ERROR Utils:563 - 
[SF_KAFKA_CONNECTOR] table name !@#@!#!@ should have at least 2 characters, start with _a-zA-Z, and only contains _$a-zA-z0-9
18-01-2023 10:27:06 main ERROR Utils:377 - 
[SF_KAFKA_CONNECTOR] snowflake.database.name cannot be empty.
18-01-2023 10:27:06 main ERROR Utils:573 - 
[SF_KAFKA_CONNECTOR] topic name topic1 is duplicated
18-01-2023 10:27:06 main ERROR Utils:448 - 
[SF_KAFKA_CONNECTOR] Kafka config:jmx should either be true or false
18-01-2023 10:27:06 main ERROR BufferThreshold:156 - 
[SF_KAFKA_CONNECTOR] buffer.flush.time is 9, it should be greater than 10
18-01-2023 10:27:06 main ERROR Utils:579 - 
[SF_KAFKA_CONNECTOR] table name table1 is duplicated
[INFO] Tests run: 55, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.494 s - in com.snowflake.kafka.connector.ConnectorConfigTest
[INFO] Running com.snowflake.kafka.connector.UtilsTest
18-01-2023 10:27:06 main INFO  Utils:212 - 
[SF_KAFKA_CONNECTOR] invalid JDBC_LOG_DIR /dummy_dir_not_exist defaulting to /tmp
18-01-2023 10:27:06 main INFO  Utils:209 - 
[SF_KAFKA_CONNECTOR] jdbc tracing directory = /usr
18-01-2023 10:27:06 main INFO  Utils:209 - 
[SF_KAFKA_CONNECTOR] jdbc tracing directory = /tmp
18-01-2023 10:27:06 main ERROR Utils:551 - 
[SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: adsadas
18-01-2023 10:27:06 main ERROR Utils:563 - 
[SF_KAFKA_CONNECTOR] table name @123 should have at least 2 characters, start with _a-zA-Z, and only contains _$a-zA-z0-9
18-01-2023 10:27:06 main INFO  Utils:114 - 
[SF_KAFKA_CONNECTOR] Current Snowflake Kafka Connector Version: 1.7.1
18-01-2023 10:27:06 main WARN  Utils:144 - 
[SF_KAFKA_CONNECTOR] Connector update is available, please upgrade Snowflake Kafka Connector (1.7.1 -> 1.8.2) 
18-01-2023 10:27:06 main ERROR Utils:551 - 
[SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: 12321
18-01-2023 10:27:06 main ERROR SnowflakeSinkTask:374 - 
[SF_KAFKA_CONNECTOR] Invalid Input, Topic2Table Map disabled
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.174 s - in com.snowflake.kafka.connector.UtilsTest
[INFO] Running com.snowflake.kafka.connector.records.RecordContentTest
[INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.083 s - in com.snowflake.kafka.connector.records.RecordContentTest
[INFO] Running com.snowflake.kafka.connector.records.MetaColumnTest
{"content":{"name":"test"},"meta":{"topic":"test","offset":0,"partition":0,"key":"test"}}
18-01-2023 10:27:06 main ERROR SnowflakeConverter:42 - 
[SF_KAFKA_CONNECTOR] Failed to parse JSON record
[SF_KAFKA_CONNECTOR] net.snowflake.client.jdbc.internal.fasterxml.jackson.core.JsonParseException: Unexpected character ('a' (code 97)): Expected space separating root-level values
[SF_KAFKA_CONNECTOR]  at [Source: (byte[])"123adsada"; line: 1, column: 5]
Config test success
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.365 s - in com.snowflake.kafka.connector.records.MetaColumnTest
[INFO] Running com.snowflake.kafka.connector.records.ValueSchemaTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.snowflake.kafka.connector.records.ValueSchemaTest
[INFO] Running com.snowflake.kafka.connector.records.HeaderTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.snowflake.kafka.connector.records.HeaderTest
[INFO] Running com.snowflake.kafka.connector.records.ConverterTest
{
  "type" : "record",
  "name" : "MyRecord",
  "fields" : [ {
    "name" : "bytesDecimal",
    "type" : {
      "type" : "bytes",
      "logicalType" : "decimal",
      "precision" : 20,
      "scale" : 4
    }
  } ]
}
18-01-2023 10:27:07 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	enhanced.avro.schema.support = false
	schemas.cache.config = 100

18-01-2023 10:27:07 main INFO  AvroConverterConfig:179 - AvroConverterConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroSerializerConfig:179 - KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroDeserializerConfig:179 - KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	enhanced.avro.schema.support = false
	schemas.cache.config = 1000

0000000001060dbba0
18-01-2023 10:27:07 main ERROR SnowflakeConverter:42 - 
[SF_KAFKA_CONNECTOR] Failed to parse JSON record
[SF_KAFKA_CONNECTOR] net.snowflake.client.jdbc.internal.fasterxml.jackson.core.JsonParseException: Unrecognized token 'fasfas': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
[SF_KAFKA_CONNECTOR]  at [Source: (byte[])"fasfas"; line: 1, column: 7]
18-01-2023 10:27:07 main ERROR SnowflakeConverter:181 - 
[SF_KAFKA_CONNECTOR] failed to parse AVRO record
[SF_KAFKA_CONNECTOR] 
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Exception: Invalid input record
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Error Code: 0010
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Detail: Input record value can't be parsed
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Message: unknown bytes
18-01-2023 10:27:07 main ERROR SnowflakeConverter:181 - 
[SF_KAFKA_CONNECTOR] failed to parse AVRO record
[SF_KAFKA_CONNECTOR] null
18-01-2023 10:27:07 main ERROR SnowflakeConverter:181 - 
[SF_KAFKA_CONNECTOR] failed to parse AVRO record
[SF_KAFKA_CONNECTOR] null
18-01-2023 10:27:07 main ERROR SnowflakeConverter:86 - 
[SF_KAFKA_CONNECTOR] Failed to parse AVRO record
[SF_KAFKA_CONNECTOR] 
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Exception: Invalid input record
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Error Code: 0010
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Detail: Input record value can't be parsed
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Message: Failed to parse AVRO record
[SF_KAFKA_CONNECTOR] [SF_KAFKA_CONNECTOR] Not an Avro data file.
18-01-2023 10:27:07 main ERROR SnowflakeConverter:95 - 
[SF_KAFKA_CONNECTOR] the string provided for reader.schema is no valid Avro schema: com.fasterxml.jackson.core.JsonParseException: Unexpected character (':' (code 58)): expected a valid value (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
[SF_KAFKA_CONNECTOR]  at [Source: (String)"{"name":"test_avro","type":"record","fields":[{"name":"int","type":"int"},{"name":"newfield","type":"int","default": 1},{"name":"missingfield","type"::"int"}]}"; line: 1, column: 152]
18-01-2023 10:27:07 main INFO  AvroConverterConfig:179 - AvroConverterConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  AvroConverterConfig:179 - AvroConverterConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  AvroConverterConfig:179 - AvroConverterConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroSerializerConfig:179 - KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroDeserializerConfig:179 - KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	enhanced.avro.schema.support = false
	schemas.cache.config = 1000

Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
18-01-2023 10:27:07 main INFO  AvroConverterConfig:179 - AvroConverterConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroSerializerConfig:179 - KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  KafkaAvroDeserializerConfig:179 - KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://fake-url]
	basic.auth.user.info = [hidden]
	proxy.host = 
	use.latest.version = false
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

18-01-2023 10:27:07 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	enhanced.avro.schema.support = false
	schemas.cache.config = 1000

18-01-2023 10:27:07 main ERROR SnowflakeConverter:181 - 
[SF_KAFKA_CONNECTOR] failed to parse AVRO record
[SF_KAFKA_CONNECTOR] null
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.186 s - in com.snowflake.kafka.connector.records.ConverterTest
[INFO] Running com.snowflake.kafka.connector.records.ProcessRecordTest
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 s - in com.snowflake.kafka.connector.records.ProcessRecordTest
[INFO] Running com.snowflake.kafka.connector.SecurityTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.002 s <<< FAILURE! - in com.snowflake.kafka.connector.SecurityTest
[ERROR] testRSAPasswordOutput(com.snowflake.kafka.connector.SecurityTest)  Time elapsed: 0.002 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.SecurityTest.testRSAPasswordOutput(SecurityTest.java:21)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.SecurityTest.testRSAPasswordOutput(SecurityTest.java:21)

[INFO] Running com.snowflake.kafka.connector.internal.FIPSTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.001 s <<< FAILURE! - in com.snowflake.kafka.connector.internal.FIPSTest
[ERROR] testFips(com.snowflake.kafka.connector.internal.FIPSTest)  Time elapsed: 0 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.FIPSTest.testFips(FIPSTest.java:20)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.FIPSTest.testFips(FIPSTest.java:20)

[INFO] Running com.snowflake.kafka.connector.internal.InternalUtilsTest
18-01-2023 10:27:07 main DEBUG InternalUtils:104 - 
[SF_KAFKA_CONNECTOR] converted date: 2019-07-18T23:32:38Z
[ERROR] Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.131 s <<< FAILURE! - in com.snowflake.kafka.connector.internal.InternalUtilsTest
[ERROR] testCreateProperties(com.snowflake.kafka.connector.internal.InternalUtilsTest)  Time elapsed: 0 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testCreateProperties(InternalUtilsTest.java:79)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testCreateProperties(InternalUtilsTest.java:79)

[ERROR] testPrivateKey(com.snowflake.kafka.connector.internal.InternalUtilsTest)  Time elapsed: 0.13 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testPrivateKey(InternalUtilsTest.java:19)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testPrivateKey(InternalUtilsTest.java:19)

[INFO] Running com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest
18-01-2023 10:27:07 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:07 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:08 main WARN  TopicPartitionChannel:590 - [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:1, message:Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
18-01-2023 10:27:08 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main WARN  TopicPartitionChannel:590 - [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:2, message:Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main WARN  TopicPartitionChannel:700 - [GET_OFFSET_TOKEN_FALLBACK] Re-opening channel:TEST_0
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:09 main WARN  TopicPartitionChannel:702 - [GET_OFFSET_TOKEN_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main ERROR TopicPartitionChannel:609 - [OFFSET_TOKEN_FALLBACK] Failed to open Channel/fetch offsetToken for channel:TEST_0
net.snowflake.ingest.utils.SFException: Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.fetchLatestCommittedOffsetFromSnowflake(TopicPartitionChannel.java:725)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.getRecoveredOffsetFromSnowflake(TopicPartitionChannel.java:706)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.streamingApiFallbackSupplier(TopicPartitionChannel.java:647)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.lambda$fetchOffsetTokenWithRetry$5(TopicPartitionChannel.java:604)
	at dev.failsafe.Functions.lambda$toFn$18(Functions.java:294)
	at dev.failsafe.internal.FallbackImpl.apply(FallbackImpl.java:58)
	at dev.failsafe.internal.FallbackExecutor.lambda$apply$0(FallbackExecutor.java:62)
	at dev.failsafe.SyncExecutionImpl.executeSync(SyncExecutionImpl.java:182)
	at dev.failsafe.FailsafeExecutor.call(FailsafeExecutor.java:438)
	at dev.failsafe.FailsafeExecutor.get(FailsafeExecutor.java:115)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.fetchOffsetTokenWithRetry(TopicPartitionChannel.java:627)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.testFetchOffsetTokenWithRetry_SFException(TopicPartitionChannelTest.java:222)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:1, startOffset:0, endOffset:0, hasErrors:true
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:260 - Received offset:0 for topic:test as the first offset for this partition:0 after start/restart/rebalance
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:222 - Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:1076, currentBufferedRecordCount:4, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=5, bufferSizeThresholdBytes=1000, bufferKafkaRecordCountThreshold=10000000}
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=4, bufferSizeBytes=1076, firstOffset=0, lastOffset=3}
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 4 records, 1076 bytes, offset 0 - 3
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:4, startOffset:0, endOffset:3, hasErrors:false
18-01-2023 10:27:09 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:09 main INFO  TopicPartitionChannel:731 - Fetched offsetToken:0 for channelName:TEST_0
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:350 - Time based flush for channel:TEST_0, CurrentTimeMs:1674037634973, previousFlushTimeMs:1674037629962, bufferThresholdSeconds:5
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=269, firstOffset=4, lastOffset=4}
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 269 bytes, offset 4 - 4
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:1, startOffset:4, endOffset:4, hasErrors:false
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:731 - Fetched offsetToken:100 for channelName:TEST_0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:260 - Received offset:0 for topic:TEST as the first offset for this partition:0 after start/restart/rebalance
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:222 - Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:14 main ERROR TopicPartitionChannel:738 - The offsetToken string does not contain a parsable long:invalidNo for channel:TEST_0
18-01-2023 10:27:14 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:14 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:15 main WARN  TopicPartitionChannel:590 - [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:1, message:Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
18-01-2023 10:27:15 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:16 main WARN  TopicPartitionChannel:590 - [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:2, message:Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
18-01-2023 10:27:16 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:700 - [GET_OFFSET_TOKEN_FALLBACK] Re-opening channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:702 - [GET_OFFSET_TOKEN_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:731 - Fetched offsetToken:0 for channelName:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:681 - [GET_OFFSET_TOKEN_FALLBACK] Channel:TEST_0, OffsetRecoveredFromSnowflake:0, Reset kafka offset to:1
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:426 - Failed Attempt to invoke the insertRows API for buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
net.snowflake.ingest.utils.SFException: Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel$InsertRowsApiResponseSupplier.get(TopicPartitionChannel.java:464)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel$InsertRowsApiResponseSupplier.get(TopicPartitionChannel.java:445)
	at dev.failsafe.Functions.lambda$toCtxSupplier$11(Functions.java:243)
	at dev.failsafe.Functions.lambda$get$0(Functions.java:46)
	at dev.failsafe.internal.FallbackExecutor.lambda$apply$0(FallbackExecutor.java:51)
	at dev.failsafe.SyncExecutionImpl.executeSync(SyncExecutionImpl.java:182)
	at dev.failsafe.FailsafeExecutor.call(FailsafeExecutor.java:438)
	at dev.failsafe.FailsafeExecutor.get(FailsafeExecutor.java:115)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertRowsWithFallback(TopicPartitionChannel.java:441)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertBufferedRecords(TopicPartitionChannel.java:385)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.testInsertRows_GetOffsetTokenFailureAfterReopenChannel(TopicPartitionChannelTest.java:420)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:700 - [INSERT_ROWS_FALLBACK] Re-opening channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:702 - [INSERT_ROWS_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main ERROR TopicPartitionChannel:432 - [INSERT_ROWS_FALLBACK] Failed to open Channel or fetching offsetToken for channel:TEST_0
net.snowflake.ingest.utils.SFException: Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.fetchLatestCommittedOffsetFromSnowflake(TopicPartitionChannel.java:725)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.getRecoveredOffsetFromSnowflake(TopicPartitionChannel.java:706)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.streamingApiFallbackSupplier(TopicPartitionChannel.java:647)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertRowsFallbackSupplier(TopicPartitionChannel.java:472)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.lambda$insertRowsWithFallback$0(TopicPartitionChannel.java:422)
	at dev.failsafe.Functions.lambda$toFn$17(Functions.java:288)
	at dev.failsafe.internal.FallbackImpl.apply(FallbackImpl.java:58)
	at dev.failsafe.internal.FallbackExecutor.lambda$apply$0(FallbackExecutor.java:62)
	at dev.failsafe.SyncExecutionImpl.executeSync(SyncExecutionImpl.java:182)
	at dev.failsafe.FailsafeExecutor.call(FailsafeExecutor.java:438)
	at dev.failsafe.FailsafeExecutor.get(FailsafeExecutor.java:115)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertRowsWithFallback(TopicPartitionChannel.java:441)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertBufferedRecords(TopicPartitionChannel.java:385)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.testInsertRows_GetOffsetTokenFailureAfterReopenChannel(TopicPartitionChannelTest.java:420)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:1, startOffset:0, endOffset:0, hasErrors:true
18-01-2023 10:27:17 main ERROR TopicPartitionChannel:501 - Insert Row Error message
net.snowflake.ingest.utils.SFException: Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.<init>(TopicPartitionChannelTest.java:68)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:250)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:260)
	at org.junit.runners.BlockJUnit4ClassRunner$2.runReflectiveCall(BlockJUnit4ClassRunner.java:309)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:260 - Received offset:0 for topic:TEST as the first offset for this partition:0 after start/restart/rebalance
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:222 - Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:1, startOffset:0, endOffset:0, hasErrors:true
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:426 - Failed Attempt to invoke the insertRows API for buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
net.snowflake.ingest.utils.SFException: Channel is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel$InsertRowsApiResponseSupplier.get(TopicPartitionChannel.java:464)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel$InsertRowsApiResponseSupplier.get(TopicPartitionChannel.java:445)
	at dev.failsafe.Functions.lambda$toCtxSupplier$11(Functions.java:243)
	at dev.failsafe.Functions.lambda$get$0(Functions.java:46)
	at dev.failsafe.internal.FallbackExecutor.lambda$apply$0(FallbackExecutor.java:51)
	at dev.failsafe.SyncExecutionImpl.executeSync(SyncExecutionImpl.java:182)
	at dev.failsafe.FailsafeExecutor.call(FailsafeExecutor.java:438)
	at dev.failsafe.FailsafeExecutor.get(FailsafeExecutor.java:115)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertRowsWithFallback(TopicPartitionChannel.java:441)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.insertBufferedRecords(TopicPartitionChannel.java:385)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.testInsertRows_SuccessAfterReopen(TopicPartitionChannelTest.java:377)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:700 - [INSERT_ROWS_FALLBACK] Re-opening channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:702 - [INSERT_ROWS_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:681 - [INSERT_ROWS_FALLBACK] Channel:TEST_0, OffsetRecoveredFromSnowflake:-1, Reset kafka offset to:0
18-01-2023 10:27:17 main WARN  TopicPartitionChannel:473 - [INSERT_ROWS_FALLBACK] Fetched offsetToken:-1 for channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main ERROR TopicPartitionChannel:780 - 
[SF_KAFKA_CONNECTOR] Failure closing Streaming Channel name:TEST_0 msg:Interrupted Exception
java.lang.InterruptedException: Interrupted Exception
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel.closeChannel(TopicPartitionChannel.java:778)
	at com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest.testCloseChannelException(TopicPartitionChannelTest.java:198)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.mockito.internal.runners.DefaultInternalRunner$1.run(DefaultInternalRunner.java:79)
	at org.mockito.internal.runners.DefaultInternalRunner.run(DefaultInternalRunner.java:85)
	at org.mockito.internal.runners.StrictRunner.run(StrictRunner.java:39)
	at org.mockito.junit.MockitoJUnitRunner.run(MockitoJUnitRunner.java:163)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:767 - Opening a channel with name:TEST_0 for table name:TEST_TABLE
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:260 - Received offset:0 for topic:test as the first offset for this partition:0 after start/restart/rebalance
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:720 - Fetching last committed offset for partition channel:TEST_0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:727 - OffsetToken not present for channelName:TEST_0
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:222 - Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:227, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:460 - Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=227, firstOffset=0, lastOffset=0}
18-01-2023 10:27:17 main DEBUG TopicPartitionChannel:975 - Get rows for streaming ingest. 1 records, 227 bytes, offset 0 - 0
18-01-2023 10:27:17 main INFO  TopicPartitionChannel:389 - [INSERT_ROWS] Successfully called insertRows for channel:TEST_0, noOfRecords:1, startOffset:0, endOffset:0, hasErrors:false
[INFO] Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.808 s - in com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest
[INFO] Running com.snowflake.kafka.connector.internal.streaming.StreamingBufferThresholdTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.snowflake.kafka.connector.internal.streaming.StreamingBufferThresholdTest
[INFO] Running com.snowflake.kafka.connector.internal.FileNameUtilsTest
18-01-2023 10:27:17 main DEBUG FileNameUtils:43 - 
[SF_KAFKA_CONNECTOR] generated file name: TEST_CONNECTOR/test_topic/123/456_789_1674037637203.json.gz
18-01-2023 10:27:17 main DEBUG FileNameUtils:74 - 
[SF_KAFKA_CONNECTOR] generated broken data file name: TEST_CONNECTOR/test_topic/123/456_key_1674037637209.gz
18-01-2023 10:27:17 main DEBUG FileNameUtils:74 - 
[SF_KAFKA_CONNECTOR] generated broken data file name: TEST_CONNECTOR/test_topic/123/456_value_1674037637209.gz
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.snowflake.kafka.connector.internal.FileNameUtilsTest
[INFO] Running com.snowflake.kafka.connector.internal.TelemetryUnitTest
18-01-2023 10:27:17 main DEBUG SnowflakeTelemetryBasicInfo:274 - 
[SF_KAFKA_CONNECTOR] Registering metrics for pipe:pipe, existing:[]
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.092 s - in com.snowflake.kafka.connector.internal.TelemetryUnitTest
[INFO] Running com.snowflake.kafka.connector.internal.LoggingTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.snowflake.kafka.connector.internal.LoggingTest
[INFO] Running com.snowflake.kafka.connector.internal.SnowflakeURLTest
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://account.snowflake.com:80
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.snowflake.com:443
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL:  account.snowflake.com:80
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: account.snowflake.com
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://account.snowflake.com 
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.snowflake.com
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.region.aws.privatelink.snowflake.com:443
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://org-account.snowflake.com:80
18-01-2023 10:27:17 main INFO  SnowflakeURL:32 - 
[SF_KAFKA_CONNECTOR] enabling JDBC tracing
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.region.aws.privatelink.snowflake.com:443
18-01-2023 10:27:17 main DEBUG SnowflakeURL:75 - 
[SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://org-account.privatelink.snowflake.com:80
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.snowflake.kafka.connector.internal.SnowflakeURLTest
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   SecurityTest.testRSAPasswordOutput:21  Runtime java.io.FileNotFoundException:...
[ERROR]   FIPSTest.testFips:20  Runtime java.io.FileNotFoundException: profile.json (No...
[ERROR]   InternalUtilsTest.testCreateProperties:79  Runtime java.io.FileNotFoundExcept...
[ERROR]   InternalUtilsTest.testPrivateKey:19  Runtime java.io.FileNotFoundException: p...
[INFO] 
[ERROR] Tests run: 147, Failures: 0, Errors: 4, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  17.967 s
[INFO] Finished at: 2023-01-18T10:27:17Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.0:test (default-test) on project snowflake-kafka-connector: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/gabsko/breaking-updates/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
